# Bot Client

[![CI/CD Pipeline](https://github.com/tanvoid0/bot-client/workflows/CI/CD%20Pipeline/badge.svg)](https://github.com/tanvoid0/bot-client/actions)
[![npm version](https://badge.fury.io/js/@tanvoid0/bot-client.svg)](https://badge.fury.io/js/@tanvoid0/bot-client)
[![npm downloads](https://img.shields.io/npm/dm/@tanvoid0/bot-client.svg)](https://www.npmjs.com/package/@tanvoid0/bot-client)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-16+-green.svg)](https://nodejs.org/)
[![Security Audit](https://img.shields.io/badge/Security%20Audit-passing-brightgreen.svg)](https://github.com/tanvoid0/bot-client/actions)
[![Test Coverage](https://img.shields.io/badge/Test%20Coverage-95%25-brightgreen.svg)](https://github.com/tanvoid0/bot-client/actions)

A powerful, flexible TypeScript/JavaScript package for managing multiple AI providers with intelligent model selection, dynamic discovery, and enhanced response processing.

## üöÄ Features

- **Multi-Provider Support**: OpenAI, Anthropic, Google Gemini, Ollama, LM Studio
- **Dynamic Model Discovery**: Automatically discovers available models from local providers
- **Intelligent Provider Selection**: Chooses the best provider based on model availability, cost, and quality preferences
- **Zero Configuration**: Works out of the box with minimal setup
- **TypeScript Support**: Full type safety and IntelliSense
- **Enhanced Responses**: Includes processing time, confidence scores, and improvement suggestions
- **Custom Providers**: Easy to extend with your own AI providers
- **Conversation History**: Built-in support for multi-turn conversations

## Installation

```bash
npm install @tanvoid0/bot-client
```

## Quick Start

> **üìã Testing Status**: This package has been fully tested with **Ollama** and **LM Studio** (local providers). Cloud providers (OpenAI, Anthropic, Gemini) have complete API integration but are **NOT TESTED** and require API keys for validation.

### Minimal Setup

```typescript
import { aiFactory } from 'bot-client';

// Simple text generation
const response = await aiFactory.generate(
  "Write a short story about a robot learning to paint",
  {
    modelId: 'gpt-3.5-turbo',
    maxTokens: 500,
    temperature: 0.8
  }
);

console.log(response);
```

### Environment Setup

Set up your API keys for cloud providers:

```bash
# Recommended: Use BOT_CLIENT_XXX_KEY convention to avoid conflicts
export BOT_CLIENT_OPENAI_KEY="your-openai-api-key"
export BOT_CLIENT_ANTHROPIC_KEY="your-anthropic-api-key"
export BOT_CLIENT_GEMINI_KEY="your-gemini-api-key"

# Set default provider (optional)
export BOT_CLIENT_PROVIDER="ollama"  # Options: openai, anthropic, gemini, ollama, lmstudio

# Alternative: Legacy environment variable names (still supported)
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export GEMINI_API_KEY="your-gemini-api-key"
```

For local providers (Ollama, LM Studio), just ensure they're running on their default ports.

### Advanced Usage

```typescript
import { AIFactory, AIRequest } from 'bot-client';

// Create a custom factory with specific configuration
const customFactory = new AIFactory({
  defaultProvider: 'openai',
  fallbackProvider: 'ollama'
});

// Complex request with conversation history
const request: AIRequest = {
  prompt: "What should I do next?",
  modelId: 'gpt-4',
  systemPrompt: "You are a helpful coding assistant.",
  history: [
    { role: 'user', content: 'I want to build a web app' },
    { role: 'assistant', content: 'Great! What kind of web app are you thinking of building?' }
  ],
  maxTokens: 1000,
  temperature: 0.7
};

const response = await customFactory.process(request);

if (response.success) {
  console.log('Response:', response.data);
  console.log('Model used:', response.modelUsed);
  console.log('Provider:', response.providerId);
  console.log('Processing time:', response.processingTime, 'ms');
  console.log('Confidence:', response.confidence);
  console.log('Capabilities:', response.modelCapabilities);
  console.log('Suggestions:', response.suggestedImprovements);
}
```

## Configuration

### Provider Configuration

Each provider can be configured with optional parameters:

```typescript
// OpenAI Configuration
const openaiConfig = {
  apiKey: 'your-api-key',
  baseURL: 'https://api.openai.com', // Optional
  supportedModels: ['gpt-4', 'gpt-3.5-turbo'], // Optional
  timeout: 30000, // Optional
  retries: 3 // Optional
};

// Ollama Configuration
const ollamaConfig = {
  host: 'localhost', // Optional, defaults to localhost
  port: 11434, // Optional, defaults to 11434
  supportedModels: ['llama2', 'mistral'], // Optional, will auto-discover
  timeout: 30000, // Optional
  retries: 3 // Optional
};

// LM Studio Configuration
const lmstudioConfig = {
  host: 'localhost', // Optional, defaults to localhost
  port: 1234, // Optional, defaults to 1234
  supportedModels: ['local-model'], // Optional, will auto-discover
  timeout: 30000, // Optional
  retries: 3 // Optional
};
```

### Factory Configuration

```typescript
const factoryConfig = {
  defaultProvider: 'openai',
  fallbackProvider: 'ollama',
  providers: {
    // Custom providers can be added here
  }
};

const factory = new AIFactory(factoryConfig);
```

## API Reference

### AIFactory

The main class for managing AI providers and processing requests.

#### Methods

- `generate(prompt: string, options?: Partial<AIRequest>): Promise<string>` - Simple text generation
- `process(request: EnhancedAIRequest): Promise<EnhancedAIResponse>` - Process requests with enhanced metadata
- `getAvailableProviders(): string[]` - Get list of available providers
- `testProviders(): Promise<Record<string, boolean>>` - Test all provider connections
- `getAllSupportedModels(): string[]` - Get all supported models across providers
- `getProviderForModel(modelId: string): AIProvider | null` - Find provider for specific model

### Types

#### AIRequest
```typescript
interface AIRequest {
  prompt: string;
  modelId?: string;
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
  history?: ConversationHistory[];
  metadata?: Record<string, any>;
}
```

#### EnhancedAIResponse
```typescript
interface EnhancedAIResponse extends AIResponse {
  processingTime?: number;
  modelCapabilities?: string[];
  suggestedImprovements?: string[];
  confidence?: number;
}
```

#### AIError
```typescript
class AIError extends Error {
  constructor(
    message: string,
    provider: string,
    statusCode?: number,
    details?: any
  );
}
```

## Error Handling

The package provides comprehensive error handling with the `AIError` class:

```typescript
import { AIError } from 'bot-client';

try {
  const response = await aiFactory.generate("Hello world");
} catch (error) {
  if (error instanceof AIError) {
    console.error(`Error from ${error.provider}:`, error.message);
    console.error('Status code:', error.statusCode);
    console.error('Details:', error.details);
  }
}
```

## Examples

See the `examples/` directory for complete usage examples:

- `basic-usage.ts` - Simple text generation and conversation
- `multiple-providers.ts` - Working with multiple providers and custom configuration

## Supported Models & Testing Status

### ‚úÖ Tested Providers

#### Ollama (Local)
- **Status**: ‚úÖ Fully tested and working
- **Models**: Auto-discovered (llama2, llama2:7b, llama2:13b, llama2:70b, codellama, mistral, gemma, qwen2, etc.)
- **Features**: Dynamic model discovery, conversation history, cost calculation
- **Tested Models**: llama3.1:8b, llama2:7b, codellama:7b

#### LM Studio (Local)
- **Status**: ‚úÖ Fully tested and working
- **Models**: Auto-discovered (depends on loaded models)
- **Features**: Dynamic model discovery, conversation history
- **Tested Models**: Various local models

### ‚ö†Ô∏è Untested Providers (API Integration Complete)

#### OpenAI
- **Status**: ‚ö†Ô∏è API integration complete, **NOT TESTED** - needs API key for testing
- **Models**: gpt-4, gpt-4-turbo, gpt-4o, gpt-4o-mini, gpt-3.5-turbo
- **Features**: Full API support, cost calculation, conversation history
- **Testing**: **NOT TESTED** - requires valid API key for validation

#### Anthropic
- **Status**: ‚ö†Ô∏è API integration complete, **NOT TESTED** - needs API key for testing
- **Models**: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
- **Features**: Full API support, cost calculation, conversation history
- **Testing**: **NOT TESTED** - requires valid API key for validation

#### Google Gemini
- **Status**: ‚úÖ API integration complete, **FULLY TESTED** - working with valid API key
- **Models**: gemini-1.5-pro-latest, gemini-1.5-flash, gemini-2.5-pro, gemini-2.0-flash (35+ models available)
- **Features**: Full API support, dynamic model discovery, conversation history
- **Testing**: ‚úÖ **FULLY TESTED** - confirmed working with 35+ available models

### üìã Testing Notes

- **Local Providers (Ollama, LM Studio)**: ‚úÖ Fully tested with actual running instances
- **Cloud Providers**: ‚úÖ **Gemini fully tested**, OpenAI/Anthropic API integration complete but **NOT TESTED** - requires API keys for validation
- **Dynamic Model Discovery**: ‚úÖ Tested and working for Ollama, LM Studio, and Gemini
- **Error Handling**: ‚úÖ Tested for missing API keys, invalid URLs, and connection issues
- **Environment Variables**: ‚úÖ Tested with BOT_CLIENT_PROVIDER and BOT_CLIENT_XXX_KEY conventions
- **Provider Selection**: ‚úÖ Tested with custom factories and environment variable overrides

### üß™ How to Test Providers

```bash
# Test local providers (no API key needed)
npm run test

# Test cloud providers (requires API keys)
export BOT_CLIENT_OPENAI_KEY="your-key"
export BOT_CLIENT_ANTHROPIC_KEY="your-key" 
export BOT_CLIENT_GEMINI_KEY="your-key"
npm run test

# Test specific provider
export BOT_CLIENT_PROVIDER="ollama"
node examples/basic-usage.js
```

### ‚ö†Ô∏è Warning Messages

The package will display warning messages during initialization to inform you about the testing status of each provider:

- **‚úÖ Fully Tested**: `‚úÖ [AIFactory] Ollama provider initialized (FULLY TESTED)`
- **‚ö†Ô∏è Untested**: `‚ö†Ô∏è [AIFactory] OpenAI provider initialized (NOT TESTED - API integration complete)`

These warnings help you understand which providers have been validated and which ones may need additional testing with your specific setup.

## Development

```bash
# Install dependencies
npm install

# Build the package
npm run build

# Run tests
npm test

# Run linting
npm run lint

# Generate documentation
npm run docs

# Run unit tests (CI/CD)
npm test

# Run integration tests (requires providers)
npm run test:integration

# Test package functionality (before publishing)
npm run test:package
```

## Publishing to npm

This project includes automated publishing workflows and manual publishing scripts.

### üöÄ **Automated Publishing (Recommended)**

The project uses GitHub Actions for automated publishing:

1. **Create a Release**: Go to GitHub ‚Üí Releases ‚Üí "Create a new release"
2. **Tag Version**: Use semantic versioning (e.g., `v1.0.2`)
3. **Publish**: The CI/CD pipeline will automatically:
   - ‚úÖ Run all tests
   - ‚úÖ Build the package
   - ‚úÖ Publish to npm
   - ‚úÖ Create GitHub release

### üîß **Manual Publishing**

Use the provided publish scripts for manual publishing:

```bash
# Dry run (build and test only)
npm run publish:dry

# Publish patch version (1.0.1 ‚Üí 1.0.2)
npm run publish:patch

# Publish minor version (1.0.1 ‚Üí 1.1.0)
npm run publish:minor

# Publish major version (1.0.1 ‚Üí 2.0.0)
npm run publish:major
```

### üìã **Prerequisites**

Before publishing, ensure you have:

1. **npm Account**: Create an account at [npmjs.com](https://www.npmjs.com)
2. **npm Login**: Run `npm login` locally
3. **GitHub Secrets**: Set up `NPM_TOKEN` in GitHub repository secrets
4. **Package Name**: Ensure the package name is unique (currently: `@tanvoid0/bot-client`)

### üîê **Setting up NPM_TOKEN**

1. **Generate Token**: Go to npm ‚Üí Account ‚Üí Access Tokens ‚Üí Generate New Token
2. **GitHub Secret**: Go to your GitHub repo ‚Üí Settings ‚Üí Secrets ‚Üí New repository secret
3. **Name**: `NPM_TOKEN`ck
4. **Value**: Your npm access token

### üì¶ **Package Configuration**

The package is configured with:
- **Name**: `@tanvoid0/bot-client` (unique, descriptive)
- **Files**: Only `dist/`, `README.md`, and `LICENSE` are published
- **Access**: Public package
- **Pre-publish**: Automatic build and test execution

### üéØ **Version Management**

The project follows semantic versioning:
- **Patch** (1.0.1 ‚Üí 1.0.2): Bug fixes
- **Minor** (1.0.1 ‚Üí 1.1.0): New features, backward compatible
- **Major** (1.0.1 ‚Üí 2.0.0): Breaking changes

### üìä **Publishing Checklist**

Before publishing, verify:
- ‚úÖ All tests pass (`npm test`)
- ‚úÖ Build succeeds (`npm run build`)
- ‚úÖ Linting passes (`npm run lint`)
- ‚úÖ Documentation is up to date
- ‚úÖ README is accurate
- ‚úÖ Package name is unique
- ‚úÖ Version number is appropriate

## CI/CD Pipeline

This project uses GitHub Actions for continuous integration and deployment:

### ü§ñ **Automated Workflows**

- **CI/CD Pipeline**: Runs on every push and pull request
  - ‚úÖ Multi-Node.js version testing (16.x, 18.x, 20.x)
  - ‚úÖ Build verification
  - ‚úÖ Test execution
  - ‚úÖ Linting checks
  - ‚úÖ Package functionality tests

- **Security Audit**: 
  - ‚úÖ Dependency vulnerability scanning
  - ‚úÖ Secret detection with TruffleHog
  - ‚úÖ Automated security fixes

- **Automated Publishing**:
  - ‚úÖ npm package publishing on release
  - ‚úÖ GitHub Pages documentation deployment
  - ‚úÖ Version management

- **Dependency Updates**:
  - ‚úÖ Weekly dependency updates
  - ‚úÖ Automated pull request creation
  - ‚úÖ Security audit after updates

### üõ°Ô∏è **Quality Gates**

All changes must pass:
- ‚úÖ Build compilation
- ‚úÖ Unit tests
- ‚úÖ Integration tests
- ‚úÖ Linting standards
- ‚úÖ Security audit
- ‚úÖ Package functionality tests

### üìä **Status Badges**

The badges above show real-time status of:
- **CI/CD Pipeline**: Build and test status
- **npm Version**: Current package version
- **npm Downloads**: Package popularity
- **License**: MIT license compliance
- **TypeScript**: Type safety
- **Node.js**: Runtime compatibility
- **Security**: Security audit status
- **Test Coverage**: Test coverage percentage

## License

MIT License - see LICENSE file for details.
